{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a7642a5",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "401c119f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mwapi\n",
    "import mwparserfromhell as parser\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dcca23",
   "metadata": {},
   "source": [
    "## Helper functions and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f6c1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://wikidata.org/w/api.php\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202af6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the pages that have a certain prefix in a given namespace\n",
    "def get_pages_with_prefix(prefix, namespace=None):\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"allpages\",\n",
    "        \"apnamespace\": namespace,\n",
    "        \"apprefix\": prefix,\n",
    "        \"aplimit\": \"max\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()  # Check for HTTP errors\n",
    "        data = response.json()\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        print(\"Response content:\", response.text)\n",
    "        return []\n",
    "\n",
    "    # if data is correctly formatted, return\n",
    "    if 'query' in data and 'allpages' in data['query']:\n",
    "        pages = data['query']['allpages']\n",
    "        return pages\n",
    "    else:\n",
    "        print(\"Unexpected response format:\", data)\n",
    "        return []\n",
    "\n",
    "# helper function for parsing external wikilinks\n",
    "def parse_external_wikilinks(link):\n",
    "    (url, text) = link.split(' ', 1)\n",
    "    text = text[:-1]\n",
    "\n",
    "    if 'query.wikidata.org/' in url:\n",
    "        query = urllib.parse.unquote(url.split('query.wikidata.org/')[1].split('#', 1)[1])\n",
    "        \n",
    "    elif 'w.wiki/' in url:\n",
    "        query = ''\n",
    "        response = requests.get(url[1:], allow_redirects=True) #url[1:] to drop the leading bracket\n",
    "        if 'query.wikidata.org/' in response.url:\n",
    "            query = urllib.parse.unquote(response.url.split('query.wikidata.org/')[1].split('#', 1)[1])\n",
    "\n",
    "    return text, query\n",
    "\n",
    "# helper functions for labeling QIDs with english labels\n",
    "def get_qid_titles(qid_set):\n",
    "    qids = {}\n",
    "    for qid in qid_set:\n",
    "        response = requests.get(f'https://www.wikidata.org/w/rest.php/wikibase/v0/entities/items/{qid}?_fields=labels')\n",
    "        data = response.json()\n",
    "        try:\n",
    "            qids[qid] = data['labels']['en']\n",
    "        except:\n",
    "            qids[qid] = ''\n",
    "    return qids\n",
    "\n",
    "# helper functions for labeling PIDs with english labels\n",
    "def get_pid_titles(pid_set):\n",
    "    pids = {}\n",
    "    for pid in pid_set:\n",
    "        response = requests.get(f'https://www.wikidata.org/w/rest.php/wikibase/v0/entities/properties/{pid}?_fields=labels')\n",
    "        data = response.json()\n",
    "        try:\n",
    "            pids[pid] = data['labels']['en']\n",
    "        except:\n",
    "            pids[pid] = ''\n",
    "    return pids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a7698c",
   "metadata": {},
   "source": [
    "# Overview\n",
    "There are several existing sources of high-quality, human-labeled, and open-source SparQL queries on Wikidata. These queries (sometimes with associated requests, titles, and discussions) can serve as a core of high-quality data for a labeled SparQL dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d402edc",
   "metadata": {},
   "source": [
    "## Query of the week (QOTW) pages\n",
    "Archive of all queries spotlighted over 7 years by the \"Query of the week\" initiative.\n",
    "\n",
    "Found at https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/qotw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beb675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_qotw_page(title):\n",
    "    print(title)\n",
    "    rows = []\n",
    "    \n",
    "    resp = session.get(\n",
    "        formatversion=2,\n",
    "        action='query',\n",
    "        prop='revisions',\n",
    "        rvslots='*',\n",
    "        rvprop='content',\n",
    "        titles=title\n",
    "    )\n",
    "\n",
    "    content = resp['query']['pages'][0]['revisions'][0]['slots']['main']['content']\n",
    "    \n",
    "    wikitext = parser.parse(content)\n",
    "\n",
    "    for l in wikitext.filter_external_links():\n",
    "        if 'query.wikidata.org/' in l or 'w.wiki/' in l:\n",
    "            try:\n",
    "                text, query = parse_external_wikilinks(l)\n",
    "                query_dict = {}\n",
    "                query_dict['heading'] = text\n",
    "                query_dict['full_text'] = ''\n",
    "                query_dict['text_query'] = ''\n",
    "\n",
    "                # get all QIDs and PIDs\n",
    "                qid_set = set(re.findall(r\"Q\\d+\", f'{text}\\n{query}'))\n",
    "                pid_set = set(re.findall(r\"P\\d+\", f'{text}\\n{query}'))\n",
    "\n",
    "                query_dict['qids'] = get_qid_titles(qid_set)\n",
    "                query_dict['pids'] = get_pid_titles(pid_set)\n",
    "                query_dict['sparql_query'] = query\n",
    "                query_dict['title'] = title\n",
    "\n",
    "                rows.append(query_dict)\n",
    "            except:\n",
    "                continue\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66467d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = mwapi.Session('https://wikidata.org', user_agent='mwapi sparql')\n",
    "qotw = []\n",
    "\n",
    "qotw_prefix = \"SPARQL query service/qotw\"\n",
    "qotw_pages = get_pages_with_prefix(qotw_prefix, namespace=4)\n",
    "for page in qotw_pages:\n",
    "    if re.search(r'Wikidata:SPARQL query service/qotw/\\d+', page['title']):\n",
    "        qotw += parse_qotw_page(page['title'])\n",
    "\n",
    "pd.DataFrame(qotw).to_pickle('data/high-quality/qotw.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe89510",
   "metadata": {},
   "source": [
    "## Request a query archive pages\n",
    "Archive of all human query requests made during the ongoing \"Request a query\" initiative.\n",
    "\n",
    "Found at https://wikidata.org/wiki/Wikidata:Request_a_query/Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad90be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a given page title, split by headings and collect:\n",
    "# - heading\n",
    "# - full text under the heading\n",
    "# - initial text query\n",
    "# - all mentions of wikidata items and properties with english labels\n",
    "# - sparql query answer\n",
    "def parse_request_a_query_page(title):\n",
    "    print(title)\n",
    "    rows = []\n",
    "    \n",
    "    resp = session.get(\n",
    "        formatversion=2,\n",
    "        action='query',\n",
    "        prop='revisions',\n",
    "        rvslots='*',\n",
    "        rvprop='content',\n",
    "        titles=title\n",
    "    )\n",
    "\n",
    "    content = resp['query']['pages'][0]['revisions'][0]['slots']['main']['content']\n",
    "    \n",
    "    wikitext = parser.parse(content)\n",
    "    headings = wikitext.filter_headings()\n",
    "\n",
    "    # split by each heading\n",
    "    for i in range(len(headings)):\n",
    "        query_dict = {}\n",
    "        \n",
    "        # get heading\n",
    "        query_dict['heading'] = headings[i]\n",
    "        \n",
    "        # get full text\n",
    "        if i == len(headings) - 1:\n",
    "            query_dict['full_text'] = wikitext.split(str(headings[i]))[1]\n",
    "        else:\n",
    "            query_dict['full_text'] = wikitext.split(str(headings[i]))[1].split(str(headings[i+1]))[0]\n",
    "\n",
    "        # get initial text query\n",
    "        text_query = query_dict['full_text'].split(\"[[User:\")[0]\n",
    "        query_dict['text_query'] = text_query\n",
    "        \n",
    "        # get all QIDs and PIDs\n",
    "        qid_set = set(re.findall(r\"Q\\d+\", query_dict['full_text']))\n",
    "        pid_set = set(re.findall(r\"P\\d+\", query_dict['full_text']))\n",
    "        query = ''\n",
    "        for template in parser.parse(query_dict['full_text']).filter_templates():\n",
    "\n",
    "            # get QIDs and PIDs that are mentioned in template form\n",
    "            if '{{Q|' in template:\n",
    "                qid = template.split('{{Q|')[1][:-2].split('|')[0]\n",
    "                if 'Q' in qid:\n",
    "                    qid_set.add(qid)\n",
    "                else:\n",
    "                    qid_set.add(f'Q{qid}')\n",
    "\n",
    "            elif '{{P|' in template:\n",
    "                pid = template.split('{{P|')[1][:-2].split('|')[0]\n",
    "                if 'P' in pid:\n",
    "                    pid_set.add(pid)\n",
    "                else:\n",
    "                    pid_set.add(f'P{pid}')\n",
    "\n",
    "            # get sparql query\n",
    "            elif '{{SPARQL' in template:\n",
    "                query = re.split(r\"query\\s*=\\s*\", str(template))[1].split(\"|\")[0]\n",
    "                if query.endswith(\"\\n}}\"):\n",
    "                    query = query[:-3]\n",
    "                if query.endswith(\"}}\"):\n",
    "                    query = query[:-2]\n",
    "                query = query.replace(\"{{!}}\", \"|\")\n",
    "\n",
    "        if query == '':\n",
    "            continue\n",
    "\n",
    "        query_dict['qids'] = get_qid_titles(qid_set)\n",
    "        query_dict['pids'] = get_pid_titles(pid_set)\n",
    "        query_dict['sparql_query'] = query\n",
    "        query_dict['title'] = title\n",
    "        \n",
    "        rows.append(query_dict)\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeff94c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "session = mwapi.Session('https://wikidata.org', user_agent='mwapi sparql')\n",
    "roq = []\n",
    "\n",
    "archive_prefix = \"Request a query\"\n",
    "archive_pages = get_pages_with_prefix(archive_prefix, namespace=4)\n",
    "for page in archive_pages:\n",
    "    if re.search(r'Wikidata:Request a query/Archive/\\d+/\\d+', page['title']):\n",
    "        roq += parse_request_a_query_page(page['title'])\n",
    "        \n",
    "pd.DataFrame(roq).to_pickle('data/high-quality/request-a-query.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f986c1e5",
   "metadata": {},
   "source": [
    "## Example SparQL query pages\n",
    "There are many example queries available on Wikidata, with headings and labels that are translated into multiple languages.\n",
    "\n",
    "Found at https://wikidata.org/wiki/Wikidata:SPARQL_query_service/queries/examples and subpages of that page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f782b08f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_example_page(title):\n",
    "    print(title)\n",
    "    rows = []\n",
    "    \n",
    "    resp = session.get(\n",
    "        formatversion=2,\n",
    "        action='query',\n",
    "        prop='revisions',\n",
    "        rvslots='*',\n",
    "        rvprop='content',\n",
    "        titles=title\n",
    "    )\n",
    "\n",
    "    content = resp['query']['pages'][0]['revisions'][0]['slots']['main']['content']\n",
    "    \n",
    "    wikitext = parser.parse(content)\n",
    "    headings = wikitext.filter_headings()\n",
    "\n",
    "    # split by each heading\n",
    "    for i in range(len(headings)):\n",
    "        query_dict = {}\n",
    "        \n",
    "        # get heading\n",
    "        query_dict['heading'] = headings[i]\n",
    "        \n",
    "        # get full text\n",
    "        if i == len(headings) - 1:\n",
    "            query_dict['full_text'] = wikitext.split(str(headings[i]))[1]\n",
    "        else:\n",
    "            query_dict['full_text'] = wikitext.split(str(headings[i]))[1].split(str(headings[i+1]))[0]\n",
    "\n",
    "        # get initial text query\n",
    "        text_query = query_dict['full_text'].split(\"{{SPARQL\")[0]\n",
    "        query_dict['text_query'] = text_query\n",
    "        \n",
    "        # get all QIDs and PIDs\n",
    "        qid_set = set(re.findall(r\"Q\\d+\", query_dict['full_text']))\n",
    "        pid_set = set(re.findall(r\"P\\d+\", query_dict['full_text']))\n",
    "        query = ''\n",
    "        for template in parser.parse(query_dict['full_text']).filter_templates():\n",
    "\n",
    "            # get QIDs and PIDs that are mentioned in template form\n",
    "            if '{{Q|' in template:\n",
    "                qid = template.split('{{Q|')[1][:-2].split('|')[0]\n",
    "                if 'Q' in qid:\n",
    "                    qid_set.add(qid)\n",
    "                else:\n",
    "                    qid_set.add(f'Q{qid}')\n",
    "\n",
    "            elif '{{P|' in template:\n",
    "                pid = template.split('{{P|')[1][:-2].split('|')[0]\n",
    "                if 'P' in pid:\n",
    "                    pid_set.add(pid)\n",
    "                else:\n",
    "                    pid_set.add(f'P{pid}')\n",
    "\n",
    "            # get sparql query\n",
    "            elif '{{SPARQL' in template:\n",
    "                query = re.split(r\"query\\s*=\\s*\", str(template))[1].split(\"|\")[0]\n",
    "                if query.endswith(\"\\n}}\"):\n",
    "                    query = query[:-3]\n",
    "                if query.endswith(\"}}\"):\n",
    "                    query = query[:-2]\n",
    "                query = query.replace(\"{{!}}\", \"|\")\n",
    "\n",
    "        if query == '':\n",
    "            continue\n",
    "\n",
    "        query_dict['qids'] = get_qid_titles(qid_set)\n",
    "        query_dict['pids'] = get_pid_titles(pid_set)\n",
    "        query_dict['sparql_query'] = query\n",
    "        query_dict['title'] = title\n",
    "        \n",
    "        rows.append(query_dict)\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce58948a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "session = mwapi.Session('https://wikidata.org', user_agent='mwapi sparql')\n",
    "example = []\n",
    "\n",
    "example_prefix = \"SPARQL query service/queries/examples\"\n",
    "example_pages = get_pages_with_prefix(example_prefix, namespace=4)\n",
    "for page in example_pages:\n",
    "    example += parse_example_page(page['title'])\n",
    "\n",
    "pd.DataFrame(example).to_pickle('data/high-quality/example.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547cca64",
   "metadata": {},
   "source": [
    "## Tweets, Facts, and Queries \n",
    "WMDE employee Lukas Werkmeister runs the Twitter and Mastodon accounts @WikidataFacts (https://twitter.com/WikidataFacts and https://mastodon.social/@WikidataFacts), which often post a tweet + a query of some kind.\n",
    "\n",
    "Lukas also has documented many (but not all!) of these queries at https://www.wikidata.org/wiki/User:TweetsFactsAndQueries/Queries and https://www.wikidata.org/wiki/User:TweetsFactsAndQueries/Problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c41535",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_tfaq_query_page(title):\n",
    "    print(title)    \n",
    "    resp = session.get(\n",
    "        formatversion=2,\n",
    "        action='query',\n",
    "        prop='revisions',\n",
    "        rvslots='*',\n",
    "        rvprop='content',\n",
    "        titles=title\n",
    "    )\n",
    "\n",
    "    content = resp['query']['pages'][0]['revisions'][0]['slots']['main']['content']\n",
    "    \n",
    "    wikitext = parser.parse(content)\n",
    "    templates = wikitext.filter_templates()\n",
    "    text = title.split('User:TweetsFactsAndQueries/Queries/')[1]\n",
    "    query_dict = {}\n",
    "    try:\n",
    "        for template in templates:\n",
    "            if 'query page' in template.name:\n",
    "                query = re.split(r\"query\\s*=\\s*\", str(template['query']))[1]\n",
    "                query = query.replace(\"{{!}}\", \"|\")\n",
    "                if template.has_param('header_wikitext_paragraph'):\n",
    "                    query_dict['text_query'] = re.split(r\"header_wikitext_paragraph\\s*=\\s*\", str(template['header_wikitext_paragraph']))[1]\n",
    "                if template.has_param('footer_wikitext'):\n",
    "                    query_dict['text_query'] += \"\\n\" + re.split(r'footer_wikitext\\s*=\\s*', str(template['footer_wikitext']))[1]\n",
    "        query_dict['heading'] = text\n",
    "        query_dict['full_text'] = ''\n",
    "\n",
    "        # get all QIDs and PIDs\n",
    "        qid_set = set(re.findall(r\"Q\\d+\", f'{text}\\n{query}'))\n",
    "        pid_set = set(re.findall(r\"P\\d+\", f'{text}\\n{query}'))\n",
    "\n",
    "        query_dict['qids'] = get_qid_titles(qid_set)\n",
    "        query_dict['pids'] = get_pid_titles(pid_set)\n",
    "        query_dict['sparql_query'] = query\n",
    "        query_dict['title'] = title\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "    return [query_dict]\n",
    "\n",
    "def parse_tfaq_problem_page(title):\n",
    "    print(title)\n",
    "    rows = []\n",
    "    \n",
    "    resp = session.get(\n",
    "        formatversion=2,\n",
    "        action='query',\n",
    "        prop='revisions',\n",
    "        rvslots='*',\n",
    "        rvprop='content',\n",
    "        titles=title\n",
    "    )\n",
    "\n",
    "    content = resp['query']['pages'][0]['revisions'][0]['slots']['main']['content']\n",
    "    \n",
    "    wikitext = parser.parse(content)\n",
    "    \n",
    "    headings = wikitext.filter_headings()\n",
    "\n",
    "    # split by each heading\n",
    "    for i in range(len(headings)):\n",
    "        if headings[i] == \"== Automatic ==\" or headings[i] == \"== Manual ==\":\n",
    "            continue\n",
    "        query_dict = {}\n",
    "        \n",
    "        # get heading\n",
    "        query_dict['heading'] = headings[i]\n",
    "        \n",
    "        # get full text\n",
    "        if i == len(headings) - 1:\n",
    "            query_dict['full_text'] = wikitext.split(str(headings[i]))[1]\n",
    "        else:\n",
    "            query_dict['full_text'] = wikitext.split(str(headings[i]))[1].split(str(headings[i+1]))[0]\n",
    "\n",
    "        # get initial text query\n",
    "        text_query = query_dict['full_text'].split(\"]\\n\")[-1]\n",
    "        query_dict['text_query'] = text_query\n",
    "        \n",
    "        for l in parser.parse(query_dict['full_text']).filter_external_links():\n",
    "            if 'query.wikidata.org/' in l or 'w.wiki/' in l:\n",
    "                try:\n",
    "                    _, query = parse_external_wikilinks(l)\n",
    "\n",
    "                    # get all QIDs and PIDs\n",
    "                    qid_set = set(re.findall(r\"Q\\d+\", f'{text_query}\\n{query}'))\n",
    "                    pid_set = set(re.findall(r\"P\\d+\", f'{text_query}\\n{query}'))\n",
    "\n",
    "                    query_dict['qids'] = get_qid_titles(qid_set)\n",
    "                    query_dict['pids'] = get_pid_titles(pid_set)\n",
    "                    query_dict['sparql_query'] = query\n",
    "                    query_dict['title'] = title\n",
    "\n",
    "                    rows.append(query_dict)\n",
    "                except:\n",
    "                    continue\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea38af9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "session = mwapi.Session('https://wikidata.org', user_agent='mwapi sparql')\n",
    "tfaq = []\n",
    "\n",
    "tfaq_prefix = \"TweetsFactsAndQueries/Queries/\"\n",
    "tfaq_pages = get_pages_with_prefix(tfaq_prefix, namespace=2)\n",
    "for page in tfaq_pages:\n",
    "    tfaq += parse_tfaq_query_page(page['title'])\n",
    "\n",
    "tfaq += parse_tfaq_problem_page('User:TweetsFactsAndQueries/Problems')\n",
    "pd.DataFrame(tfaq).to_pickle('data/high-quality/tfaq.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f43053",
   "metadata": {},
   "source": [
    "### Tweets, facts, and queries Twitter archive\n",
    "\n",
    "I managed to get the full archive of tweets/retweets directly from Lucas. They're now stored at `data/WikidataFacts-tweets.jsonl`. This section is for parsing those tweets + descriptions + links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8b3d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# helper function for parsing external wikilinks\n",
    "def parse_tweet_links(url):\n",
    "    if 'query.wikidata.org/' in url:\n",
    "        query = urllib.parse.unquote(url.split('query.wikidata.org/')[1].split('#', 1)[1])\n",
    "        \n",
    "    elif 'w.wiki/' in url or 'tinyurl.com/' in url:\n",
    "        query = ''\n",
    "        response = requests.get(url, allow_redirects=True)\n",
    "        if 'query.wikidata.org/' in response.url:\n",
    "            query = urllib.parse.unquote(response.url.split('query.wikidata.org/')[1].split('#', 1)[1])\n",
    "\n",
    "    return query\n",
    "\n",
    "def parse_tweet(t):\n",
    "    rows = []\n",
    "    for u in t['tweet']['entities']['urls']:\n",
    "        if 'query.wikidata.org/' in u['expanded_url'] or 'w.wiki/' in u['expanded_url'] or 'tinyurl.com/' in u['expanded_url']:\n",
    "            try:\n",
    "                query_dict = {}\n",
    "                query = parse_tweet_links(u['expanded_url'])\n",
    "                if t['tweet']['full_text']:\n",
    "                    text = t['tweet']['full_text']\n",
    "                    query_dict['heading'] = text\n",
    "                else:\n",
    "                    continue\n",
    "                query_dict['full_text'] = ''\n",
    "                query_dict['text_query'] = ''\n",
    "\n",
    "                # get all QIDs and PIDs\n",
    "                qid_set = set(re.findall(r\"Q\\d+\", f'{text}\\n{query}'))\n",
    "                pid_set = set(re.findall(r\"P\\d+\", f'{text}\\n{query}'))\n",
    "\n",
    "                query_dict['qids'] = get_qid_titles(qid_set)\n",
    "                query_dict['pids'] = get_pid_titles(pid_set)\n",
    "                query_dict['sparql_query'] = query\n",
    "                query_dict['title'] = '@WikidataFacts_Twitter_Archive'\n",
    "\n",
    "                rows.append(query_dict)\n",
    "            except:\n",
    "                continue\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e513c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "\n",
    "with open('data/WikidataFacts-tweets.jsonl', 'r') as f:\n",
    "    for l in f:\n",
    "        t = json.loads(l)\n",
    "        tweets += parse_tweet(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f7886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(tweets).to_pickle('data/high-quality/tweets.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8cdab7",
   "metadata": {},
   "source": [
    "## Weekly query example pages\n",
    "Sometimes Wikidata status updates (https://www.wikidata.org/wiki/Wikidata:Status_updates) include descriptive links to queries, which are aggregated in the Weekly query example archive (https://www.wikidata.org/wiki/Wikidata:Weekly_query_examples). This archive goes back almost 10 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09a8f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_weekly_query_page(title):\n",
    "    print(title)\n",
    "    rows = []\n",
    "    \n",
    "    resp = session.get(\n",
    "        formatversion=2,\n",
    "        action='query',\n",
    "        prop='revisions',\n",
    "        rvslots='*',\n",
    "        rvprop='content',\n",
    "        titles=title\n",
    "    )\n",
    "\n",
    "    content = resp['query']['pages'][0]['revisions'][0]['slots']['main']['content']\n",
    "    \n",
    "    wikitext = parser.parse(content)\n",
    "\n",
    "    for l in wikitext.filter_external_links():\n",
    "        if 'query.wikidata.org/' in l or 'w.wiki/' in l:\n",
    "            try:\n",
    "                text, query = parse_external_wikilinks(l)\n",
    "                query_dict = {}\n",
    "                query_dict['heading'] = text\n",
    "                query_dict['full_text'] = ''\n",
    "                query_dict['text_query'] = ''\n",
    "\n",
    "                # get all QIDs and PIDs\n",
    "                qid_set = set(re.findall(r\"Q\\d+\", f'{text}\\n{query}'))\n",
    "                pid_set = set(re.findall(r\"P\\d+\", f'{text}\\n{query}'))\n",
    "\n",
    "                query_dict['qids'] = get_qid_titles(qid_set)\n",
    "                query_dict['pids'] = get_pid_titles(pid_set)\n",
    "                query_dict['sparql_query'] = query\n",
    "                query_dict['title'] = title\n",
    "\n",
    "                rows.append(query_dict)\n",
    "            except:\n",
    "                print('error')\n",
    "                continue\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbf69d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = mwapi.Session('https://wikidata.org', user_agent='mwapi sparql')\n",
    "weekly_query = []\n",
    "\n",
    "weekly_query_prefix = \"Weekly query examples\"\n",
    "weekly_query_pages = get_pages_with_prefix(weekly_query_prefix, namespace=4)\n",
    "for page in weekly_query_pages:\n",
    "    weekly_query += parse_weekly_query_page(page['title'])\n",
    "\n",
    "pd.DataFrame(weekly_query).to_pickle('data/high-quality/weekly-query.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c853fef5",
   "metadata": {},
   "source": [
    "## User pages with a suffix of \"query\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b709ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tktk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30acda8",
   "metadata": {},
   "source": [
    "## Aggregate all data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c1067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qotw = pd.read_pickle('data/high-quality/qotw.pkl')\n",
    "request = pd.read_pickle('data/high-quality/request-a-query.pkl')\n",
    "example = pd.read_pickle('data/high-quality/example.pkl')\n",
    "tfaq = pd.read_pickle('data/high-quality/tfaq.pkl')\n",
    "weekly = pd.read_pickle('data/high-quality/weekly-query.pkl')\n",
    "tweets = pd.read_pickle('data/high-quality/tweets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9b7113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df(df):\n",
    "    df['heading'] = df['heading'].astype('string')\n",
    "    return df\n",
    "\n",
    "dfs = [qotw, request, example, tfaq, weekly, tweets]\n",
    "dfs = [format_df(df) for df in dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89051d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pd = pd.concat(dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d0d1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pd.to_pickle('data/high-quality/all.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa4a62d",
   "metadata": {},
   "source": [
    "### Drop non-english translations of queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8e7d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = [\n",
    "    'de', 'es', 'nl', 'es', 'fr', 'hy', 'ja', 'sv', 'tr', 'uk',\n",
    "    'zh', 'ar', 'be-tarask', 'ca', 'cs', 'da', 'eo', 'eu', 'he',\n",
    "    'arz', 'id', 'it', 'pl', 'sv', 'th', 'ko', 'lt', 'ms', 'pt-br',\n",
    "    'ru', 'ro', 'si', 'vec'\n",
    "]\n",
    "\n",
    "suffixes = [f'/{s}' for s in translated_suffixes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08692a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_no_translation = all_pd[\n",
    "    ~all_pd.apply(\n",
    "        lambda row: any(row.str.endswith(suffix).any() for suffix in suffixes), axis=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497f7907",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_no_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fe6e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_no_translation.to_pickle('data/high-quality/all_no_translation.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092cdbb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07fd2536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84aec8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/high-quality/all_no_translation.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06cbbd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2e5b953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['heading', 'full_text', 'text_query', 'qids', 'pids', 'sparql_query',\n",
       "       'title'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04f98cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Women who have been elected to the National Academy of Sciences'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.text_query == '']['heading'][400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d38c7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Lexemes by number of statements ===\n"
     ]
    }
   ],
   "source": [
    "print(df['heading'][5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9c68871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3824d0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unsupported pickle protocol: 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sn/nd2vlwp52w94trc8v9nb46bc0000gp/T/ipykernel_4739/3903086136.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/high-quantity/wikidata-sparql-templates-bug-fixes.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/wiki-sparql-data-creation/venv/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0;31m# RawIOBase, BufferedIOBase, TextIOBase, TextIOWrapper, mmap]\";\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                     \u001b[0;31m# expected \"IO[bytes]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mexcs_to_catch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0;31m# e.g.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unsupported pickle protocol: 5"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('data/high-quantity/wikidata-sparql-templates-bug-fixes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf665d47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
