{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a94ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mwapi mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da33b83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mwapi\n",
    "from mwapi.errors import APIError\n",
    "import mwparserfromhell as parser\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d7818",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_transcluded_pages(session, template):\n",
    "    continued = session.get(\n",
    "        formatversion=2,\n",
    "        action='query',\n",
    "        prop='transcludedin',\n",
    "        titles=f\"Template:{template}\",\n",
    "        continuation=True\n",
    "    )\n",
    "\n",
    "    pages = []\n",
    "    try:\n",
    "        for portion in continued:\n",
    "            if 'query' in portion:\n",
    "                for page in portion['query']['pages']:\n",
    "                    try:\n",
    "                        for transcluded in page['transcludedin']:\n",
    "                            pages.append(transcluded[\"title\"])\n",
    "                    except:\n",
    "                        pass\n",
    "            else:\n",
    "                print(\"MediaWiki returned empty result batch.\")\n",
    "    except APIError as error:\n",
    "        raise ValueError(\n",
    "            \"MediaWiki returned an error:\", str(error)\n",
    "        )\n",
    "    \n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2e09c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sparql(session, p, t):\n",
    "    resp = session.get(\n",
    "        formatversion=2,\n",
    "        action='query',\n",
    "        prop='revisions',\n",
    "        rvslots='*',\n",
    "        rvprop='content',\n",
    "        titles=p\n",
    "    )\n",
    "\n",
    "    content = resp['query']['pages'][0]['revisions'][0]['slots']['main']['content']\n",
    "    wikitext = parser.parse(content)\n",
    "    templates = wikitext.filter_templates()\n",
    "    templates = list(filter(lambda template: t in template, templates))\n",
    "    if t == \"Wikidata list\":\n",
    "        templates = list(filter(lambda template: template != \"{{Wikidata list end}}\", templates))\n",
    "    \n",
    "    out = []\n",
    "    for template in templates:\n",
    "        out.append(template.split(\"|\")[1].split(\"=\")[1])\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa90cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_templates(template):\n",
    "    for t in templates:\n",
    "        if t in template:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def split_string_and_extract_preceding(s, delimiter):\n",
    "    parts = s.split(delimiter)  # Split the string by the delimiter.\n",
    "    preceding_texts = []  # Initialize a list to hold the preceding text segments.\n",
    "    \n",
    "    search_pos = 0  # Start position for each search iteration.\n",
    "    for part in parts[:-1]:  # Ignore the last part since no split occurs after it.\n",
    "        # Calculate the start position of the current part in the original string.\n",
    "        current_part_start = s.find(part, search_pos)\n",
    "        # Calculate the end position of the current part, which is the split point.\n",
    "        split_point = current_part_start + len(part)\n",
    "        \n",
    "        # Determine the start position for extracting preceding characters.\n",
    "        # It's the greater of 0 and split_point - 300 to avoid negative indices.\n",
    "        extract_start = max(0, split_point - 300)\n",
    "        \n",
    "        # Extract up to 250 characters preceding the split point.\n",
    "        preceding_text = s[extract_start:split_point]\n",
    "        preceding_texts.append(preceding_text)\n",
    "        \n",
    "        # Update the search position for the next iteration.\n",
    "        search_pos = split_point + len(delimiter)\n",
    "    \n",
    "    return preceding_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192ea88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparql_and_surrounding(title):\n",
    "    out = []\n",
    "    resp = session.get(\n",
    "        formatversion=2,\n",
    "        action='query',\n",
    "        prop='revisions',\n",
    "        rvslots='*',\n",
    "        rvprop='content',\n",
    "        titles=title\n",
    "    )\n",
    "    content = resp['query']['pages'][0]['revisions'][0]['slots']['main']['content']\n",
    "    wikitext = parser.parse(content)\n",
    "    wikitext_templates = list(filter(check_templates, wikitext.filter_templates()))\n",
    "    wikitext_templates = list(filter(lambda template: template != \"{{Wikidata list end}}\", wikitext_templates))\n",
    "    if '{{query page' in wikitext:\n",
    "        lede = wikitext[:250]\n",
    "        query = re.split(\"query\\s*=\\s*\", str(wikitext))[1].split(\"|\")[0]\n",
    "        text = None\n",
    "        results = None\n",
    "        out.append({\"title\": title, \"lede\": lede, 'preceding_text': text, 'query': query, 'results': results})\n",
    "    elif len(wikitext_templates) > 0:\n",
    "        for wt in wikitext_templates:\n",
    "            lede = wikitext[:250]\n",
    "            text = split_string_and_extract_preceding(wikitext, str(wt))\n",
    "            results = None\n",
    "            if \"wdquery\" in wt.lower():\n",
    "                query = re.split(\"query\\s*=\\s*\", str(wt))[1].split(\"|\")[0]\n",
    "            elif \"complex constraint\" in wt.lower():\n",
    "                lede = re.split(\"label\\s*=\\s*\", str(wt))[1].split(\"|\")[0]\n",
    "                text = re.split(\"description\\s*=\\s*\", str(wt))[1].split(\"|\")[0]\n",
    "                query = re.split(\"sparql\\s*=\\s*\", str(wt))[1].split(\"|\")[0]\n",
    "            elif \"wikidata list\" in wt.lower():\n",
    "                ts = wikitext.find(str(wt))\n",
    "                te = wikitext.lower().find(\"{{wikidata list end}}\")\n",
    "                truncated = wikitext[ts:te]\n",
    "                results = truncated[truncated.find(\"{|\"):truncated.find(\"|}\")]\n",
    "                query = re.split(\"=\\s*\", str(wt))[1].split(\"|\")[0]\n",
    "            else:\n",
    "                query = wt.split(\"|\")[1].split(\"=\", 1)[1]\n",
    "            out.append({\"title\": title, \"lede\": lede, 'preceding_text': text, 'query': query, 'results': results})\n",
    "        return out\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba772c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = [\n",
    "    \"Wikidata list\",\n",
    "    \"SPARQL\",\n",
    "    \"SPARQL2\",\n",
    "    \"SPARQL5\",\n",
    "    \"SPARQL Inline\",\n",
    "    \"Wdquery\",\n",
    "    \"Complex constraint\"\n",
    "]\n",
    "\n",
    "template_regex_string = \"|\".join([f\"{{{{\\s*[{t[0].lower()}|{t[0].upper()}]{t[1:]}\\s*\\|\" for t in templates])\n",
    "\n",
    "wikis = set()\n",
    "\n",
    "with open('wikis.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        wikis.add(f'https://{line[:-1]}')\n",
    "\n",
    "big_wikis = [\n",
    "    'https://en.wikipedia.org',\n",
    "    'https://fr.wikipedia.org',\n",
    "    'https://de.wikipedia.org',\n",
    "    'https://ja.wikipedia.org',\n",
    "    'https://ru.wikipedia.org',\n",
    "    'https://pt.wikipedia.org',\n",
    "    'https://it.wikipedia.org',\n",
    "    'https://zh.wikipedia.org',\n",
    "    'https://fa.wikipedia.org',\n",
    "    'https://ar.wikipedia.org',\n",
    "    'https://commons.wikimedia.org',\n",
    "    'https://wikidata.org',\n",
    "    'https://mediawiki.org'\n",
    "]\n",
    "\n",
    "wikis.update(big_wikis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8001baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['project', 'title', 'lede', 'preceding_text', 'query', 'results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f6b937",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for w in wikis:\n",
    "    fail_ctr = 0\n",
    "    print(w)\n",
    "    session = mwapi.Session(w, user_agent=\"htriedman sparql corpus bot\")\n",
    "    all_pages = set()\n",
    "    for t in templates:\n",
    "        pages = get_transcluded_pages(session, t)\n",
    "        print(f'template {t} occurs {len(pages)} times on {w}')\n",
    "        all_pages.update(pages)\n",
    "    print(f'there are a total of {len(all_pages)} sparql-related pages on {w}')  \n",
    "    for i, p in enumerate(all_pages):\n",
    "        if i % 500 == 0:\n",
    "            print(f'templates seen: {i}')\n",
    "        try:\n",
    "            out = get_sparql_and_surrounding(p)\n",
    "            if out is None:\n",
    "                continue\n",
    "            out[0]['project'] = w\n",
    "            df = pd.concat([df, pd.DataFrame.from_dict(out)])\n",
    "        except:\n",
    "            fail_ctr += 1\n",
    "            if fail_ctr % 50 == 0 and fail_ctr != 0:\n",
    "                print(f'failures: {fail_ctr}')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a441b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# failure-prone wikis: commons, cswiki, cawiki, nowiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac7ab33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d7786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('wikidata-sparql-templates.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d0d1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
